{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c57431a-fa82-4346-8041-508b38f52e3b",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07db0939-57f3-484a-96f9-58df87d7e3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import platform\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as sk\n",
    "import torch\n",
    "\n",
    "has_gpu = torch.cuda.is_available()\n",
    "has_mps = torch.backends.mps.is_built()\n",
    "custom_device = \"mps\" if has_mps else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Python Platform: {platform.platform()}\")\n",
    "print(f\"PyTorch Version: {torch.__version__}\\n\")\n",
    "print(f\"Python {sys.version}\")\n",
    "print(f\"Pandas {pd.__version__}\")\n",
    "print(f\"Numpy {np.__version__}\")\n",
    "print(f\"Scikit-Learn {sk.__version__}\")\n",
    "print(\"NVIDIA/CUDA GPU is\", \"available\" if has_gpu else \"NOT AVAILABLE\")\n",
    "print(\"MPS (Apple Metal) is\", \"AVAILABLE\" if has_mps else \"NOT AVAILABLE\")\n",
    "\n",
    "print(f\"\\nCustome Device:\\t{custom_device}\")\n",
    "\n",
    "# Option 1 on Mac (with Apple Silicon) is to use the CPU:\n",
    "#torch.set_default_device(\"cpu\") # <- setting it manually to \"cpu\"\n",
    "\n",
    "# Option 2 on Mac (with Apple Silicon) is to use MPS:\n",
    "torch.set_default_device(custom_device)\n",
    "\n",
    "print(f\"Active device:\\t{torch.get_default_device()}\")\n",
    "\n",
    "# Testing\n",
    "print(\"\\nRun test:\")\n",
    "layer = torch.nn.Linear(20,30)\n",
    "print(f\"\\tLayer weights are on device: {layer.weight.device}\")\n",
    "print(f\"\\tLayer creating data on device: {layer(torch.randn(128,20)).device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a85c98-1e1e-4813-9519-f3f77c910ce9",
   "metadata": {},
   "source": [
    "# Classification (of images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff89615-3fdb-4e24-a3dc-112ee9b43234",
   "metadata": {},
   "source": [
    "## Creating random data for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8931ba82-ebad-444d-af95-cec1c95d25e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "\n",
    "# variables for num of samples\n",
    "n_samples = 1000\n",
    "\n",
    "# creating circles (noise defines randomness; with random_state a random seed is set)\n",
    "X, y = make_circles(n_samples, noise=0.03, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727afbbe-da3e-4b57-8a39-0bd02c0f455e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X), len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2d7cd2-993a-4e26-b4b0-1b2710d121dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X[:5], y[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb273c26-fef9-48b5-ae08-24313a18cc7f",
   "metadata": {},
   "source": [
    "## Visualizing the created data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa563e59-31eb-4f39-a432-ef1b16f69f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\"X1\": X[:,0],\n",
    "        \"X2\": X[:,1],\n",
    "        \"label\": y}\n",
    "\n",
    "circles = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73623c3-dbd8-4020-8dda-a9e24368b74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "circles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8608db-50ec-42c1-b837-235ca35a1856",
   "metadata": {},
   "outputs": [],
   "source": [
    "circles[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3c890d-cefb-4335-a5fe-411e6f5db427",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "plt.scatter(x=circles[\"X1\"],y=circles[\"X2\"],c=y, cmap=plt.cm.RdYlBu);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0330aacb-8e1a-41e2-835d-bb84238a419d",
   "metadata": {},
   "source": [
    "## Transforming ndarray into Torch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017b4ff3-ba6a-4fc9-954a-7b98f02b1962",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.from_numpy(X).type(torch.float).to(custom_device)\n",
    "y = torch.from_numpy(y).type(torch.float).to(custom_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bb43d1-ea81-4593-a673-29f17e5a083c",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X), type(y), X.dtype, y.dtype, X.device, y.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debff065-1842-4945-8ad7-a923d9237ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d7cbf8-e9a4-48d5-b102-ef36c8a0f567",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training Data:\\n\\tNum of Features\\t\\t{len(X_train)}\\n\\tNum of Labels\\t\\t{len(y_train)}\\n\\nTesting Data:\\n\\tNum of Features\\t\\t{len(X_test)}\\n\\tNum of Labels\\t\\t{len(y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1acb9d-61fd-4353-a877-c74176a9dab2",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5b0406-8abf-4368-8857-4fe045eefbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer_1 = torch.nn.Linear(in_features=2, out_features=64)\n",
    "        self.layer_2 = torch.nn.Linear(in_features=64, out_features=64)\n",
    "        self.layer_3 = torch.nn.Linear(in_features=64, out_features=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Structure: x -> layer_1 -> layer_2 -> output\n",
    "        return self.layer_3(self.layer_2(self.layer_1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97913f51-413f-40aa-b9ed-bc41a7642c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiation of model\n",
    "model_0 = ClassificationModel()\n",
    "\n",
    "# model summary\n",
    "print(model_0.state_dict())\n",
    "print(f\"\\nModel structure:\\n{model_0}\")\n",
    "print(f\"\\nDevice:\\t{next(model_0.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2ec3d3-7501-4916-af89-0d901747d400",
   "metadata": {},
   "source": [
    "## Model replication with torch.nn.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892e224d-d79e-4143-8f93-4a75f43a20f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_0 = torch.nn.Sequential(\n",
    "    torch.nn.Linear(in_features=2, out_features=64),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(in_features=64, out_features=64),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(in_features=64, out_features=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e892553-b44f-4463-a665-12a606821898",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(model_0.state_dict())\n",
    "print(f\"\\nModel structure:\\n{model_0}\")\n",
    "print(f\"\\nDevice:\\t{next(model_0.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129e77cb-a661-4061-8c35-5958bdae984b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_0.eval()\n",
    "with torch.inference_mode():\n",
    "    untrained_preds = model_0(X_test)\n",
    "\n",
    "print(f\"Length of preds: {len(untrained_preds)}, Shape: {untrained_preds.shape}\")\n",
    "print(f\"Length of samples: {len(X_test)}, Shape: {X_test.shape}\")\n",
    "print(f\"\\nFirst 5 preds:\\n{untrained_preds[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85255f7-2754-4708-819c-95208b8c836f",
   "metadata": {},
   "source": [
    "## Loss Function & Optimizer\n",
    "\n",
    "Classification problems are basically devided into binary and multi-class classification problems. <p>Hence, using either `torch.nn.BCELossWithLogits` (with built-in *sigmoid* activation function) or `torch.nn.BCELoss` for the former and `torch.nn.CrossEntropyLoss` for the latter as **Loss Function** is the logical choice. <p> Either \"SGD\" (`torch.optim.SGD()`) or \"Adam\" (`torch.optim.Adam()`) is used as **Optimizer**.\n",
    "\n",
    "Official PyTorch Documentation\n",
    "* [Loss functions overview](https://docs.pytorch.org/docs/stable/nn.html#loss-functions)\n",
    "* [Optimizer overview](https://docs.pytorch.org/docs/stable/optim.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839f1525-a216-4f77-b8bc-94ddb17efc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "loss_function = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(params=model_0.parameters(),\n",
    "                            lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd8b3c9-a4e9-45b7-8357-145290edad9c",
   "metadata": {},
   "source": [
    "## Accuracy Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526d6671-733a-4969-bb6e-bf1b9fbadf40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_function(y_true, y_preds):\n",
    "    correct = torch.eq(y_true,y_preds).sum().item()\n",
    "    accuracy = (correct / len(y_preds)) * 100\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40885d0e-1c10-4634-b298-a734d043710e",
   "metadata": {},
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8d0f00-45e2-411d-91ec-73d6770b37f5",
   "metadata": {},
   "source": [
    "### Peeking on the raw logits\n",
    "\n",
    "Why inspect logits?\n",
    "\t•\tDebugging scale and distribution: You might want to make sure your scores aren’t exploding (e.g. all in the thousands) or collapsing to a very narrow band near zero.\n",
    "\t•\tMargin analysis: In binary classification, a logit near 0 means the model is “unsure,” whereas a large-magnitude logit (positive or negative) shows strong confidence.\n",
    "\t•\tRelative ordering: For multi-class, the largest logit determines the predicted class even before softmax, so you can check whether the model’s top-k order makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f357c4e4-cfbb-4bc1-8fc3-b74359cf36b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_0.eval()\n",
    "with torch.inference_mode():\n",
    "    # logits (y_logits) are the raw, unnormalized outputs of a model before activation functions like softmax are applied \n",
    "    y_logits = model_0(X_test)[:5]\n",
    "\n",
    "y_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252a3acd-752f-4897-bebd-d14efa9a2909",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_labels = torch.round(torch.sigmoid(model_0(X_test[:5])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe3d5e9-5599-4e02-9a54-4e95e5102d45",
   "metadata": {},
   "source": [
    "### Training & Test Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2109d9d8-9dd7-40a6-ae21-ea76d22b6202",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mps.manual_seed(42) # or `torch.manual_seed(42)`\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model_0.train()\n",
    "\n",
    "    # FORWARD PROPAGATION\n",
    "    y_logits = model_0(X_train).squeeze()\n",
    "    y_preds = torch.round(torch.sigmoid(y_logits))\n",
    "\n",
    "    # LOSS (loss function requires raw logits as input)\n",
    "    loss = loss_function(y_logits,\n",
    "                         y_train)\n",
    "    accuracy = accuracy_function(y_true=y_train,\n",
    "                                 y_preds=y_preds)\n",
    "\n",
    "    # OPTIMIZER\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # BACKPROPAGATION\n",
    "    loss.backward()\n",
    "\n",
    "    # GRADIENT DESCENT\n",
    "    optimizer.step()\n",
    "\n",
    "    # TESTING\n",
    "    model_0.eval()\n",
    "    with torch.inference_mode():\n",
    "        # FORWARD PROPAGATION\n",
    "        test_logits = model_0(X_test).squeeze()\n",
    "        test_preds = torch.round(torch.sigmoid(test_logits))\n",
    "\n",
    "        # LOSS / ACCURACY CALCULATION\n",
    "        test_loss = loss_function(test_logits,\n",
    "                                  y_test)\n",
    "\n",
    "        test_accuracy = accuracy_function(y_true=y_test,y_preds=test_preds)\n",
    "\n",
    "\n",
    "    if (epoch + 1) % (epochs / 10) == 0:\n",
    "        #epoch_count.append(epoch)\n",
    "        #loss_values.append(loss)\n",
    "        #test_loss_values.append(test_loss)\n",
    "        print(f\"Epoch {epoch+1}/{epochs} | Loss: {loss.item():.4f} | Accuracy: {accuracy:.2f}% | Test Loss: {test_loss.item():.4f} | Test Accuracy: {test_accuracy:.2f}%\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af2f81b-a88a-4396-829a-825dce33f530",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pathlib import Path \n",
    "\n",
    "# Download helper functions from Learn PyTorch repo (if not already downloaded)\n",
    "if Path(\"helper_functions.py\").is_file():\n",
    "  print(\"helper_functions.py already exists, skipping download\")\n",
    "else:\n",
    "  print(\"Downloading helper_functions.py\")\n",
    "  request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")\n",
    "  with open(\"helper_functions.py\", \"wb\") as f:\n",
    "    f.write(request.content)\n",
    "\n",
    "from helper_functions import plot_predictions, plot_decision_boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d75b8dc-808a-4f67-acde-7a79c316a756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot decision boundaries for training and test sets\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Train\")\n",
    "plot_decision_boundary(model_0, X_train, y_train)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Test\")\n",
    "plot_decision_boundary(model_0, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfb0c71-d3f6-473e-9c7a-28a71e733298",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (torch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
